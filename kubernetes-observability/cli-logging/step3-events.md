Events are the collections of logs generated when events occur in a Kubernetes cluster. These are not your application events, on your application data plane. Instead, these are the journal of changes to the cluster, on the control plane. Kubernetes administrators will be keenly interested in these log details to get insight into resource events on the cluster.

`kubectl get events`{{execute}}

In the listing, you can see events related to you starting the application during the previous step. These are the Create, read, update, and delete ([CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete)) events for the container lifecycle, but not your specific application activity.

The listing only reveals events associated with activities specific to the `default` namespace. To see events related to the cluster components specify a namespace, such as `kube-system`.

`kubectl get events --namespace=kube-system`{{execute}}

These cluster events are stored in _etcd_ and managed by Kubernetes. Because these events accumulate, the older ones are automatically purged. The typical default is one hour, and there is a setting called _time-to-live_ that on some clusters can be adjusted through the `kube-apiserver --event-ttl`. This time is kept short because if too many events accumulate within the _time-to-live_ period, it's possible _etcd_ can overfill. Additional tools can stream these events to other channels such as ElasticSearch and persistent data stores.

## Events of Resources

Every resource can have associated events. These are the significant state change events related to when the resource is created, updated, or removed. For instance, take a peak as the last few lines of the description of the namespace `default`.

`kubectl describe namespace default`{{execute}}

See how Kubernetes has associated the scaling of the random-logger deployment as first 1, then when you scaled it to three. The same events are associated with the deployment.

`kubectl describe deployment/random-logger`{{execute}}

## Event Router

There are many tools for routing these events. As an example, Heptio maintains a simple [Eventrouter](https://github.com/heptiolabs/eventrouter) service.

> The event router serves as an active watcher of event resource in the Kubernetes system, which takes those events and pushes them to a user-specified sync. This is useful for different purposes, but most notably long term behavioral analysis of your workloads running on your Kubernetes cluster.

Give it a try, install Eventrouter.

`kubectl create -f https://raw.githubusercontent.com/heptiolabs/eventrouter/master/yaml/eventrouter.yaml`{{execute}}

In a moment, inspect the Eventrouter log and to see the type of data it will sync to other sources (if the sync was to be configured).

`kubectl logs -f deployment/eventrouter -n kube-system --limit-bytes=2048`{{execute}}

## Deeper into Kubelet and Linux Weeds

The Kubelet is the primary “node agent” that runs on each node. It's not a container, it's a Linux process that is core to Kubernetes.

`ps -lC "kubelet"`{{execute}}

On this Katacoda cluster, kubelet is managed with systemd and can be listed.

`systemctl --no-pager status kubelet`{{execute}}

There is sometimes additional information in the journal entries for kubelets.

`journalctl --no-pager --lines=3 --unit kubelet`{{execute}}

On other clusters where systemd is not used, the Kubernetes components that make up the control plane will produce logs in /var/log on each cluster node. In this Katacoda instance, there are two directories for `pods` and `containers`.

`ls -ld /var/log/*/`{{execute}}

> [Root hog or die](https://en.wikipedia.org/wiki/Root_hog_or_die)

It's not often you want to be _rooting_ around in these logs across all the nodes in your cluster, but it may be something to keep in mind when diagnosing hard to find issues.

These are the log entries related to the cluster and generated by components on the Kubernetes control plane. These events are unrelated to application log events, which you will explore next.
